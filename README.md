# ğŸ™ï¸ Public Speaking AI

AI-powered platform for analyzing public speaking skills through **audio and video** signals â€” providing real-time, multimodal feedback on delivery, tone, and body language.  
Built with **FastAPI**, **Python**, and integrated with a **Next.js frontend**.

---

## ğŸ§  Overview

This system runs **three independent FastAPI servers** that work together:

| Component | Folder | Environment | Port | Description |
|------------|---------|-------------|-------|--------------|
| ğŸ§ **Audio Analysis API** | `/Audio` | `venv` | `8080` | Extracts pitch, energy, filler words, pauses |
| ğŸ¥ **Video Analysis API** | `/Video` | `/Video/v_env` | `8002` | Analyzes posture, gestures, and facial cues |
| ğŸ”— **Fusion API** | `/Fusion-api` | `fusion-env` | `9000` | Combines both outputs into unified feedback |

---

## âš™ï¸ Prerequisites

- **Python** 3.10+
- **Node.js** 18+ (if connecting to frontend)
- **pip** and **virtualenv**
- **ffmpeg** installed (for audio/video processing)
- (Optional) **ngrok** for public tunneling to Vercel frontend

---

## ğŸ§© Environment Setup

### 1ï¸âƒ£ Audio Environment â€” `/venv`

```bash
cd "C:\Users\chint\OneDrive\Desktop\Public Speaking AI"
python -m venv venv
venv\Scripts\Activate.ps1
pip install -r requirements_audio.txt
```

#### Run the Audio API
```bash
cd Audio
 uvicorn Fast-api-app.main:app --reload --port 8080
```

#### Core Dependencies
```
fastapi
uvicorn
librosa
numpy
whisper
pandas
soundfile
scikit-learn
xgboost
pydub
```

---

### 2ï¸âƒ£ Video Environment â€” `/Video/v_env`

```bash
cd "C:\Users\chint\OneDrive\Desktop\Public Speaking AI\Video"
python -m venv v_env
v_env\Scripts\Activate.ps1
pip install -r requirements_video.txt
```

#### Run the Video API
```bash
cd Video
uvicorn app:app --reload --port 8002
```

#### Core Dependencies
```
fastapi
uvicorn
opencv-python
mediapipe
numpy
pandas
matplotlib
fer
torch
torchvision
```

---

### 3ï¸âƒ£ Fusion Environment â€” `/fusion-env`

```bash
cd "C:\Users\chint\OneDrive\Desktop\Public Speaking AI"
python -m venv fusion-env
fusion-env\Scripts\activate
pip install -r requirements_fusion.txt
```

#### Run the Fusion API
```bash
cd Fusion-api
uvicorn fusion:app --reload --port 9000
```

#### Core Dependencies
```
fastapi
uvicorn
httpx
asyncio
shutil
pathlib
pydantic
```

---

## ğŸ”— FastAPI Endpoints

### ğŸ§ Audio API (`localhost:8080`)
**POST** `/analyze-audio`  
**Input:** `.wav` or `.mp4`  
**Output Example:**
```json
{
  "status": "success",
  "pitch_variance": 0.34,
  "energy_variance": 0.29,
  "filler_count": 5,
  "audio_score": "good"
}
```

---

### ğŸ¥ Video API (`localhost:8002`)
**POST** `/analyze/`  
**Input:** `.mp4`  
**Output Example:**
```json
{
  "status": "success",
  "posture_score": "good",
  "gesture_activity": "moderate",
  "emotion": "confident"
}
```

---

### ğŸ”— Fusion API (`localhost:9000`)
**POST** `/analyze/`  
Combines both analyses into a single JSON response.

**Example Request:**
```bash
curl -X 'POST'   'http://127.0.0.1:9000/analyze/'   -H 'accept: application/json'   -H 'Content-Type: multipart/form-data'   -F 'file=@demo.mp4;type=video/mp4'
```

**Example Response:**
```json
{
  "status": "success",
  "input_video": "uploads/demo.mp4",
  "audio_analysis": {
    "pitch_variance": 0.34,
    "filler_count": 5,
    "audio_score": "good"
  },
  "video_analysis": {
    "gesture_activity": "moderate",
    "emotion": "confident"
  }
}
```

---

## ğŸ“ Folder Structure

```
Public Speaking AI/
â”‚
â”œâ”€â”€ Audio/
â”‚   â”œâ”€â”€ Acoustic/
â”‚   â”œâ”€â”€ analyze_audio_final.py
â”‚
â”œâ”€â”€ Video/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ multimodal_pipeline.py
â”‚   â”œâ”€â”€ v_env/
â”‚   â””â”€â”€ requirements_video.txt  
â”‚
â”œâ”€â”€ Fusion-api/
â”‚   â”œâ”€â”€ fusion_main.py
â”‚
â”œâ”€â”€ ORA-Speaker(Copy)
â”‚
â”œâ”€â”€ fusion-env/
â”œâ”€â”€ venv/
â”œâ”€â”€ requirements_audio.txt
â””â”€â”€ requirements_fusion.txt

```

---

## ğŸ§¾ Notes

- All three APIs are **independent** â€” Fusion just coordinates them.  
- Keep environment paths correct (`venv`, `Video/v_env`, `fusion-env`).
- Use unique ports (`8080`, `8002`, `9000`).
- Swagger docs:
  - Audio â†’ `http://127.0.0.1:8080/docs`
  - Video â†’ `http://127.0.0.1:8002/docs`
  - Fusion â†’ `http://127.0.0.1:9000/docs`

---

## ğŸ§‘â€ğŸ’» Contributors â€” Team ORA Farmer

- [@Nishant-coder-cpu](https://github.com/Nishant-coder-cpu)  
- [@Karancoderg](https://github.com/Karancoderg)  
- [@Harman-2005](https://github.com/Harman-2005)  
- [@Coding-yeager](https://github.com/Coding-yeager)  
- [@Ayush18-pixel](https://github.com/Ayush18-pixel)



---

## ğŸªª License
MIT License Â© 2025 Public Speaking AI
